

---


# Apache Kafka Basics: Self-Learning Project

This project is a self-learning exercise to understand and implement the core concepts of **Apache Kafka** using Docker, Python producers/consumers, and message partitioning.

---

## üß† What I Learned

- Running Kafka and Zookeeper using Docker Compose
- Writing Python-based Kafka **producers** and **consumers**
- Understanding **topic partitioning**, **consumer groups**, and **key-based message routing**
- Using the **Faker** library to generate mock data for messages
- Observing how Kafka distributes messages across partitions and consumers

---

## üß± Project Structure

```
.
‚îú‚îÄ‚îÄ compose.yml
‚îú‚îÄ‚îÄ producer.py
‚îú‚îÄ‚îÄ consumer.py
‚îú‚îÄ‚îÄ producer_partitioned.py
‚îú‚îÄ‚îÄ consumer_1.py
‚îú‚îÄ‚îÄ consumer_2.py
‚îî‚îÄ‚îÄ consumer_3.py
```

---

## üöÄ How to Run

### 1. Start Kafka and Zookeeper

```bash
docker compose up -d
```

> Uses Confluent Kafka 7.2.1

---

### 2. Basic Producer & Consumer Example

#### Run the producer
```bash
python producer.py
```

Sends random user metadata to topic: `metadata-topic`

#### Run the consumer
```bash
python consumer.py
```

Reads from the same topic and prints user data.

---

### 3. Partitioned Producer & Consumers

#### Run the partitioned producer
```bash
python producer_partitioned.py
```

Sends categorized messages (e.g., `sports`, `tech`, `politics`) to topic: `multi-partition-topic` using category as the **key**.

#### Run the consumers (in separate terminals)

```bash
python consumer_1.py
python consumer_2.py
python consumer_3.py
```

Each consumer joins the same **consumer group** (`partition-demo-group`), and Kafka load-balances partitions across them.

---

## üîç Key Concepts Demonstrated

- **Topics**: `metadata-topic`, `multi-partition-topic`
- **Partitioning**: Key-based message routing to partitions
- **Consumer Groups**: Demonstrated with multiple consumers balancing the workload
- **Serialization**: JSON messages with `Faker` data
- **Docker Setup**: Minimal `compose.yml` for Kafka and Zookeeper

---

## üì¶ Dependencies

Install the required libraries:

```bash
pip install kafka-python faker
```

---

## üßπ Cleanup

To stop and remove Kafka/Zookeeper containers:

```bash
docker compose down --remove-orphans
```

To forcibly remove Kafka containers:

```bash
docker rm -f kafka-basics-kafka-1 kafka-basics-zookeeper-1
```






```

