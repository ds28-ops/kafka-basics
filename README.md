

---

```markdown
# Apache Kafka Basics: Self-Learning Project

This project is a self-learning exercise to understand and implement the core concepts of **Apache Kafka** using Docker, Python producers/consumers, and message partitioning.

---

## ğŸ§  What I Learned

- Running Kafka and Zookeeper using Docker Compose
- Writing Python-based Kafka **producers** and **consumers**
- Understanding **topic partitioning**, **consumer groups**, and **key-based message routing**
- Using the **Faker** library to generate mock data for messages
- Observing how Kafka distributes messages across partitions and consumers

---

## ğŸ§± Project Structure

```
.
â”œâ”€â”€ compose.yml
â”œâ”€â”€ producer.py
â”œâ”€â”€ consumer.py
â”œâ”€â”€ producer_partitioned.py
â”œâ”€â”€ consumer_1.py
â”œâ”€â”€ consumer_2.py
â””â”€â”€ consumer_3.py
```

---

## ğŸš€ How to Run

### 1. Start Kafka and Zookeeper

```bash
docker compose up -d
```

> Uses Confluent Kafka 7.2.1

---

### 2. Basic Producer & Consumer Example

#### Run the producer
```bash
python producer.py
```

Sends random user metadata to topic: `metadata-topic`

#### Run the consumer
```bash
python consumer.py
```

Reads from the same topic and prints user data.

---

### 3. Partitioned Producer & Consumers

#### Run the partitioned producer
```bash
python producer_partitioned.py
```

Sends categorized messages (e.g., `sports`, `tech`, `politics`) to topic: `multi-partition-topic` using category as the **key**.

#### Run the consumers (in separate terminals)

```bash
python consumer_1.py
python consumer_2.py
python consumer_3.py
```

Each consumer joins the same **consumer group** (`partition-demo-group`), and Kafka load-balances partitions across them.

---

## ğŸ” Key Concepts Demonstrated

- **Topics**: `metadata-topic`, `multi-partition-topic`
- **Partitioning**: Key-based message routing to partitions
- **Consumer Groups**: Demonstrated with multiple consumers balancing the workload
- **Serialization**: JSON messages with `Faker` data
- **Docker Setup**: Minimal `compose.yml` for Kafka and Zookeeper

---

## ğŸ“¦ Dependencies

Install the required libraries:

```bash
pip install kafka-python faker
```

---

## ğŸ§¹ Cleanup

To stop and remove Kafka/Zookeeper containers:

```bash
docker compose down --remove-orphans
```

To forcibly remove Kafka containers:

```bash
docker rm -f kafka-basics-kafka-1 kafka-basics-zookeeper-1
```

---

## ğŸ§  Notes

- The `version` key in `compose.yml` is deprecated for Docker Compose v2+.
- Ensure your Kafka service is available at `localhost:9092` when running Python scripts.

---

## ğŸ“š Further Learning

- Explore custom partitioners
- Implement message acknowledgment and manual offset commits
- Integrate Kafka with databases or visualization tools

---

## ğŸ§‘â€ğŸ’» Author

Self-learning by **Dhruv Sridhar**
```

